{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary health classifier(ham_spam).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvNggD54jBQNtxUVGobvYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Binary-text-classifier-ham_spam-/blob/main/Binary_health_classifier(ham_spam).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofygcuHSWEGY",
        "outputId": "476fe181-9ede-43af-8735-7acf90457f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'binary_text_classifier'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 14 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sudeshmu/binary_text_classifier.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import pickle as pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM, Embedding, Input, Activation, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "dataset_file_path = \"/content/binary_text_classifier/spam.csv\"\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "train_test_ratio = 0.8\n",
        "train_batch_size=128\n",
        "n_epochs=10\n",
        "\n",
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model\n",
        "\n",
        "def train_n_test ():\n",
        "    data = pd.read_csv (dataset_file_path, delimiter=',', encoding='latin-1')\n",
        "    X = data.v2\n",
        "    y = data.v1\n",
        "    \n",
        "    label_encoder = LabelEncoder ()\n",
        "    y = label_encoder.fit_transform (y)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split (X, y, \n",
        "                                                         train_size=train_test_ratio, \n",
        "                                                         shuffle=True)\n",
        "    tok = Tokenizer(num_words=max_words)\n",
        "    tok.fit_on_texts(X_train)\n",
        "    sequences = tok.texts_to_sequences(X_train)\n",
        "    sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "    \n",
        "    model = RNN()\n",
        "    model.summary()\n",
        "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(sequences_matrix,y_train,batch_size=train_batch_size,epochs=n_epochs,\n",
        "              validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "    \n",
        "    print (\"Training Completed\")\n",
        "    \n",
        "    test_sequences = tok.texts_to_sequences(X_test)\n",
        "    test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "    \n",
        "    accr = model.evaluate(test_sequences_matrix,y_test)\n",
        "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "    \n",
        "\n",
        "train_n_test ()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oDFxEIGWI45",
        "outputId": "458c50ad-c472-4321-a728-d2d9e4239ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs (InputLayer)         [(None, 150)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 150, 50)           50000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                29440     \n",
            "                                                                 \n",
            " FC1 (Dense)                 (None, 256)               16640     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " out_layer (Dense)           (None, 1)                 257       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,337\n",
            "Trainable params: 96,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "28/28 [==============================] - 9s 219ms/step - loss: 0.3385 - accuracy: 0.8642 - val_loss: 0.1204 - val_accuracy: 0.9563\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 5s 193ms/step - loss: 0.0972 - accuracy: 0.9764 - val_loss: 0.0220 - val_accuracy: 0.9933\n",
            "Training Completed\n",
            "35/35 [==============================] - 1s 21ms/step - loss: 0.0864 - accuracy: 0.9767\n",
            "Test set\n",
            "  Loss: 0.086\n",
            "  Accuracy: 0.977\n"
          ]
        }
      ]
    }
  ]
}